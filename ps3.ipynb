{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 3: Neural Networks\n",
    "\n",
    "### Overview\n",
    "In this problem set, you'll explore the foundational concepts of neural networks, covering basic architecture, activation functions, forward propagation, regularization, and adaptive learning. You'll also complete coding exercises to implement neural network components and train a basic neural network. You will both complete this Python script (`ps3.ipynb`) and submit a LaTeX report (`ps3.tex` and `ps3.pdf`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Architecture of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is composed of layers of neurons: input layer, hidden layer(s), and an output layer. Each neuron receives inputs, processes them, and passes the result to the next layer.\n",
    "\n",
    "In this section, you'll implement a basic 3-layer neural network structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weights and biases for each layer\n",
    "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)  # Input to Hidden layer\n",
    "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)  # Hidden to Output layer\n",
    "        self.bias_output = np.zeros((1, self.output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Forward propagation\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = relu(self.hidden_input)  # Hidden layer activation\n",
    "        \n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = sigmoid(self.output_input)  # Output layer activation (for binary classification)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Mean Squared Error loss function (for regression)\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        # Backward propagation (Gradient Descent)\n",
    "        \n",
    "        # Output layer error and gradient\n",
    "        output_error = y_true - self.output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # Hidden layer error and gradient\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * relu_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        \n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "# Example usage:\n",
    "input_size = 3  # Example: 3 input features\n",
    "hidden_size = 4  # Hidden layer with 4 neurons\n",
    "output_size = 1  # Output layer (1 neuron for binary classification)\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Example input data (X) and labels (y)\n",
    "X = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.4, 0.5, 0.6],\n",
    "              [0.7, 0.8, 0.9]])\n",
    "y = np.array([[0], [1], [0]])\n",
    "\n",
    "# Forward propagation\n",
    "output = nn.forward(X)\n",
    "print(\"Output after forward propagation:\")\n",
    "print(output)\n",
    "\n",
    "# Compute loss\n",
    "loss = nn.compute_loss(y, output)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# Backpropagation (for a single training step)\n",
    "nn.backward(X, y, learning_rate=0.1)\n",
    "\n",
    "# Output after one training step\n",
    "output_after_training = nn.forward(X)\n",
    "print(\"Output after one training step:\")\n",
    "print(output_after_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Activation functions introduce non-linearity into the network, allowing it to learn more complex patterns. Common functions include:\n",
    "\n",
    "- **Sigmoid**: S-shaped curve, useful for binary classifications.\n",
    "- **ReLU (Rectified Linear Unit)**: Outputs zero if input is negative, otherwise outputs the input.\n",
    "- **Leaky ReLU**: Similar to ReLU but with a small gradient for negative inputs.\n",
    "\n",
    "### Exercise:\n",
    "Write functions for each activation function and plot them over a range of inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation is the process of passing inputs through the network to generate an output.\n",
    "\n",
    "### Exercise:\n",
    "Implement forward propagation for a 3-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting occurs when a model learns the noise in the training data rather than the actual pattern. Regularization helps control this.\n",
    "\n",
    "Common techniques include:\n",
    "\n",
    "- **L2 Regularization**: Adds a penalty based on the sum of squared weights.\n",
    "- **Dropout**: Randomly ignores some neurons during training.\n",
    "\n",
    "### Exercise:\n",
    "Implement L2 regularization in the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training involves adjusting the weights and biases to minimize the loss. A popular method is gradient descent.\n",
    "\n",
    "### Exercise:\n",
    "Implement a simple gradient descent loop to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adaptive Learning Rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive learning rate methods, like Adam, adjust the learning rate based on past gradients, improving training stability.\n",
    "\n",
    "### Exercise:\n",
    "Implement a simple version of the Adam optimizer for gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
